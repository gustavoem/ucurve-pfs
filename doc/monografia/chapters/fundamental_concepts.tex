% Contextualização e conceitos fundamentais
% .1 O problema de seleção de características
% .2 Funções de custo para esse problema
% .3 Redução para o problema de curvas em U

\section{O problema de seleção de características}
A seleção de características é um problema de otimização combinatória 
em que procuramos o melhor subconjunto de um conjunto de características
$S$. O espaço de busca desse problema é o conjunto potência de $S$, 
$\powerset (S)$, que é a coleção de todos os subconjuntos possíveis de
 $S$. A função de custo desse problema é uma função $c : \powerset (S) 
\to \fieldR_{+}$.

\begin{mydefinition}[Problema de seleção de características] Seja $S$
um conjunto de características, finito e não vazio, e $c$ uma função de 
custo. Encontrar $X \in \powerset (S)$ tal que $c (X) \leq c (Y)$,
$\forall Y \in \powerset (S)$.
\end{mydefinition}

O espaço de busca do problema de seleção de características possui uma
relação de ordem parcial definida pela relação $\subseteq$, portanto
este conjunto é {\bf parcialmente ordenado (poset)}.

\begin{mydefinition}
Uma {\bf \em cadeia} do reticulado booleano é uma sequência $X_1$, 
$X_2$, ..., $X_l$ tal que $X_1 \subseteq X_2 \subseteq \dots 
\subseteq X_l$.
\end{mydefinition}


%No contexto de aprendizado de máquina, é comum que as funções de custo
%utilizadas na seleção de característica descrevam curvas próximas do 
%formato de u nas cadeias do reticulado. Esse fenômeno é conhecido em 
%aprendizado e explicaremos como ele ocorre na seção 
%\ref{fund_concept:mce}.

%Nesta seção apresentaremos as duas funções de custo mais utilizadas 
%durante este trabalho: a entropia condicional média (MCE) e a soma de 
%subconjuntos. A primeira foi utilizada na seleção de modelos de
%aprendizado, enquanto a segunda foi utilizada para criação e solução
%de instâncias artificiais. 
%A função de soma de subconjuntos é 
%decomponível em curvas u e a MCE não, porém explicaremos como a última 
%função deve ter um formato parecido com a da curva em u.

\section{Funções de custo}
\label{fund_concept:mce} A função de custo utilizada na 
solução do problema deve, de alguma forma, refletir a qualidade do 
conjunto de características avaliado. Por isso,
diferentes aplicações de seleção de características
podem ter diferentes funções de custo. No contexto de aprendizado de 
máquina, uma possível função de custo é a entropia condicional média
(MCE), que já foi utilizada por exemplo na construção de W-operadores
~\cite{MJCJB06}.

\begin{mydefinition}\label{def:conditional_entropy}
Dado um problema de aprendizado em que $Y$ é o conjunto de possíveis
rótulos e $W = (w_1, ..., w_n)$, com $w_i \in A_i$, é o conjunto de
variáveis. Seja $W' = (w_{I(1)}, w_{I(2)}, ..., w_{I(k)})$ um conjunto 
de variáveis (características) escolhidas, $\mathbf{X}$ uma vetor 
aleatório de tamanho $k$ com ${X_j} \in A_{I(j)}$, e $log0 = 0$. Então,
a {\bf \em entropia condicional} de $Y$ dado $\mathbf{X} = \mathbf x$ é:

\begin{center}
$
\begin{aligned}
H (Y | \mathbf{X = x}) = - 
\sum_{y \in Y} \probability (Y = y | \mathbf{X = x}) log \probability (Y = y | \mathbf{X = x}).
\end{aligned}
$
\end{center}
\end{mydefinition}

\begin{mydefinition}
Sob o mesmo contexto definido em \ref{def:conditional_entropy}, 
definimos a {\bf \em entropia condicional média} como:
\begin{center}
$
\begin{aligned}
    \expectation[H (Y | \mathbf{X})] = 
    \sum_{\mathbf{x} \in \mathbf{X}} H (Y | \mathbf{X = x}) \probability (\mathbf{X = x}).
\end{aligned}
$
\end{center}
\end{mydefinition}

%Vamos usar esta função como exemplo para entender intuitivamente como
%as funções de custo no problema de seleção de características descrevem
%curvas em u.

A função $H$, em teoria da informação, mede o inverso da quantidade 
média de informação que uma variável tem. Esta função atinge valor 
máximo quando a distribuição de probabilidade da variável aleatória em
questão é uniforme (todos valores que ela pode assumir são 
equiprováveis), e tem valores baixos quando essa distribuição é 
concentrada. 

Problemas de aprendizado em que os rótulos tem uma distribuição 
concentrada são mais fáceis do que os problemas em que essa distribuição
é menos concentrada. Tome como exemplo o problema de
classificar o lançamento de uma moeda $\mathbf{x}$ em $y$ (cara ou 
coroa); se toda moeda $\mathbf{x}$ é não viciada, então a distribuição
de $\probability (y | \mathbf{x})$ é pouco concentrada, por outro lado,
quando a moeda é viciada, a distribuição de 
$\probability (y | \mathbf{x})$ é concentrada e é mais fácil 
classificar este problema. Em termos mais formais, o erro do melhor 
classificador do problema mais fácil é menor do que o erro do melhor 
classificador do problema mais difícil.

Portanto, como a função $H$ é capaz de medir a concentração da 
distribuição de $Y$ dado $\mathbf{X = x}$, e quanto maior esta 
concentração mais fácil é o modelo de aprendizado, podemos dizer
que a função de custo $\expectation[H (Y | \mathbf{X})]$ pode 
representar a qualidade do modelo de classificação que usa o conjunto de
características de $\mathbf{X}$.

Agora, como já entendemos o funcionamento da função de custo
MCE e como ela se relaciona com a qualidade do conjunto de 
características avaliado, vamos entender o que acontece no modelo de 
aprendizado e na função de custo que usamos como exemplo quando 
percorremos uma cadeia do reticulado. 

Uma cadeia do poset pode ser vista como uma sequência de possíveis
escolhas de conjuntos de características ao qual a cada passo 
adicionamos uma característica. Isso significa que a cada passo dado
a variável $\mathbf{x}$ ganha uma componente a mais. Quando estamos no 
início da cadeia, poucas variáveis do problema são consideradas, 
portanto há uma grande abstração dos dados dos objetos sendo 
classificados, e conforme subimos uma cadeia, diminuímos a abstração dos
dados e isso faz com que a distribuição de $Y$ dado $\mathbf{x}$ se 
concentre.

Essa concentração da distribuição da probabilidade indica que o custo 
dos subconjuntos deve diminuir conforme subimos por uma cadeia do 
reticulado, e este raciocínio nos leva a pensar que adicionar 
características sempre melhora a classificação; de fato, o valor de
$\expectation[H (Y | \mathbf{X})]$ deve diminuir (até algum ponto 
de saturação) conforme aumentamos o número de variáveis do problema. 
Mas se isso é verdade, por que fazemos seleção de características? A 
inconsistência entre esse raciocínio e a motivação para seleção de 
característica é que essa linha de raciocínio negligenciou o fato de que 
problemas de classificação (supervisionada) dependem de uma amostra da 
distribuição de $Y$ dado $\mathbf{X = x}$, ou seja, não sabemos nem ao 
menos calcular $H (Y | \mathbf{X = x})$, podemos apenas estimar o seu 
valor a partir da amostra.

A amostra da distribuição de $Y$ dado $\mathbf{X = x}$ é obtida do 
conjunto de treinamento do problema de aprendizado e quando o número
de amostras não é grande o suficiente a qualidade do classificador 
é comprometida. Além disso, o número de amostras necessárias deve
crescer conforme aumentamos a complexidade do modelo de aprendizado 
utilizado. Considerando que quando subimos uma cadeia do reticulado 
booleano estamos aumentando a complexidade do modelo, temos que, a
partir de um certo ponto, a qualidade do classificador que utiliza tal 
conjunto de características deve piorar. 

Portanto, é esperado que a função de custo descreva um formato de U nas 
cadeias do reticulado. No começo da cadeia, o custo deve diminuir por 
conta da maior granularidade dos dados de entrada, até algum ponto onde
a limitação no número de amostras combinada com o aumento da 
complexidade do modelo causem erros de estimação que aumentam o erro
do classificador criado em tal modelo.

No cálculo da entropia condicional média, o efeito do aumento da 
complexidade de $\mathbf X$ é a estimação ruim de 
$\probability (Y = y | \mathbf{X = x})$. Contorna-se este problema 
modificando a entropia condicional média para penalizar a entropia de 
$Y$ quando $\mathbf{x}$ foi observado poucas vezes. A função de custo
utilizada é, então:

\begin{equation} \label{cost_function:mce}
    \hat{\expectation}[H (Y | \mathbf{X})] = \frac{N}{t}
    \sum_{\mathbf{x} \in \mathbf{X}} H (Y | \mathbf{X = x}) \probability (\mathbf{X = x}).
\end{equation}

\section{O problema U-curve}
A função de custo apresentada na seção ~\ref{fund_concept:mce}
descrevem curvas que tem um formato em U (a menos de oscilações) nas 
cadeias do reticulado booleano, vamos definir esta propriedade agora.

\begin{mydefinition}
Uma cadeia é dita {\bf \em maximal} se não existe outra cadeia no 
reticulado que contenha propriamente esta cadeia.
\end{mydefinition}

\begin{mydefinition}\label{fund_concepts:ushape}
Uma função de custo $c$ é dita {\bf \em decomponível em curvas U} se
para toda cadeia maximal $X_1, ..., X_l$, $c(X_j) \leq max \{c (X_i),
c (X_k)\}$ sempre que $X_i \subseteq X_j \subseteq X_k$, $i, j, k \in 
\{1, ..., l\}$.
\end{mydefinition}

Vamos considerar então o problema de seleção de características em que a
função de custo utilizada é decomponível em curvas U. Este é o problema 
central deste trabalho.

\begin{mydefinition}[Problema U-Curve]
Dados um conjunto finito e não-vazio $S$ e uma função de custo $c$ 
decomponível em curvas em U, encontrar um subconjunto $X \in 
\powerset (S)$ tal que $c(X) \leq c(Y)$,  $\forall Y \in \powerset (S)$.
\end{mydefinition}

O problema U-Curve é um caso particular do problema de seleção de 
características com uma propriedade que nos permite achar o mínimo
global sem a necessidade de avaliar cada ponto do reticulado booleano. 
Isso é possível porque a propriedade U-Curve (da decomponibilidade da 
função de custo em curvas U) nos garante que o custo dos elementos de 
uma cadeia não podem cair uma vez que aumentaram. Sejam por exemplo
dois elementos $A \subseteq B$ de $\powerset (S)$, então:
\begin{itemize}
    \item{se $c(B) > c (A)$, então $c (X) > c (A)$ para todo $X$
        do intervalo $[B, \powerset (S)]$;}
    \item{se $c(A) > c (B)$, então $c (X) > c (B)$ para todo $X$ 
        do intervalo $[\emptyset, A]$.}
\end{itemize}

Desta maneira, quando um problema de seleção de características tem uma 
função de custo decomponível em curvas U a menos de algumas oscilações,
é vantajoso aproximar a solução deste problema pela solução encontrada
por um algoritmo de busca do problema U-Curve. Tal abordagem não é 
ótima, porém, como existem poucas oscilações da função de custo, é 
provável que a solução encontrada ainda seja próxima da melhor solução.

\section{Funções de custo artificiais}
Para testar e comparar o desempenho de algoritmos precisamos de muitas 
instâncias do problema U-Curve. Usar instâncias reais para este fim pode
ser inviável devido a escassez de dados e, além disso, é possível isto
cause uma análise viesada para os dados usados. Por isso é necessário 
usar funções de custo que nos permitam criar instâncias artificiais 
parecidas com problemas reais e que sejam tão gerais quanto possível, 
evitando que a avaliação dos algoritmos seja viesada. 

Nesta seção apresentamos duas funções de custo artificiais, a primeira 
foi utilizada na maioria dos testes de desempenho dos algoritmos, 
servindo de base para avaliar tempo de execução, otimalidade e número de 
chamadas da função custo. A segunda não foi utilizada em testes deste
trabalho, mas foi objeto de estudos pois, diferente da primeira, é capaz
de simular violações da hipótese de curva U, o que é esperado que ocorra
mesmo que moderadamente em instâncias reais.

\subsection{Soma de subconjuntos}
Para se avaliar o desempenho dos algoritmos criados neste trabalho, 
utilizamos instâncias artificiais que são reduções do problema da soma
de subconjuntos. Este problema consiste em, dado um conjunto finito de
inteiros não-negativos $S$ e um inteiro não-negativo $t$, descobrir se
há um subconjunto de $S$ que soma $t$. Podemos resolver este problema 
com a solução de uma instância do problema de seleção de características
onde o conjunto de características é $S'$ uma cópia de $S$ e a função de
custo é $c$:

\begin{equation} \label{cost_function:subset_sum}
    c (X) = |t - \sum_{x \in X} x| \text{, para todo } 
                                        X \in \powerset(S') \text{.}
\end{equation}

Assim como a função de custo MCE, a função de custo de somas de 
subconjuntos também apresenta formato de curva em U nas cadeias
do reticulado Booleano~\cite{Rei12}. Para toda cadeia com elementos 
$A \subseteq B 
\subseteq C$ vale que $c (B) \leq max\{c (A), c (B)\}$, então de fato 
esta função é decomponível em curvas U. Vamos apresentar a prova desta propriedade, feita em~\cite{Rei12}. Começamos a 
demonstração definindo $D = B \setminus A$ e $E = C \setminus B$. Este problema tem dois casos disjuntos:  $|t - \sum_{b \in B}
b| > 0$ ou então $|t - \sum_{b \in B} b| \leq 0$.  

\begin{itemize}
    \item{se $|t - \sum_{b \in B} b| > 0$, então:}
    \begin{align*}
        c (B) & =  |t - \sum_{b \in B} b|  & \\
              & \leq  |t - \sum_{b \in B} b + \sum_{d \in D} d| & 
                \text{(pois $S$ contém apenas números positivos e $t -
                \sum_{b \in B} b > 0$)} \\
              & = |t - \sum_{a \in B \setminus D} a| \\
              & = |t - \sum_{a \in A} a| \\
              & = c (A).
    \end{align*}
    Portanto, $c (B) \leq  c (A)$, logo $c (B) \leq max \{c (A), c (C)\}$.
    
    \item{se $|t - \sum_{b \in B} b| \leq 0$, então:}
    \begin{align*}
        c (B) & =  |t - \sum_{b \in B} b|  & \\
              & \leq  |t - \sum_{b \in B} b - \sum_{e \in E} e| & 
                \text{(pois $S$ contém apenas números positivos e $t -
                \sum_{b \in B} b \leq 0$)} \\
              & = |t - \sum_{c \in B \cup E} c| \\
              & = |t - \sum_{c \in C} c| \\
              & = c (C).
    \end{align*}
    Portanto, $c (B) \leq  c (C)$, logo $c (B) \leq max \{c (A), c (C)\}$.
\end{itemize}
Como acabamos de provar para os dois casos possíveis, temos que 
$c (B) \leq max \{c (A), c (C)\}$. Desta maneira, a função de custo de
soma de subconjuntos é decomponível em curvas U.\qed

\subsection{Violação da curva U}
Como vimos na seção ~\ref{def:conditional_entropy}, funções de custo 
utilizadas em problemas de seleção de características podem descrever 
curvas em formato de U nas cadeias do reticulado Booleano, e isto nos 
permite aproximar a solução destes problemas assumindo que a função de 
custo é decomponível em curvas U. Entretanto, é possível que as funções
de custo apresentem oscilações, ou seja, mínimos locais. Por isso, é 
interessante estudar a robustez dos algoritmos que resolvem o problema 
U-Curve quando a hipótese de curva em U não é verdadeira. Apesar de não 
analisarmos os algoritmos deste trabalho quanto sua robustez, achamos 
importante citar esta métrica de qualidade.

Uma maneira de gerar funções com oscilações e violações da curva em U
é adicionar uma perturbação senoidal a uma função decomponível em curvas
U. Seja a seguinte função:

\begin{equation*} 
    c(X | X_0, {\mathbf W}, c_{max}) = c_{max} [1 - e^{- \frac{1}{2} (X - X_0)^T {\mathbf W} (X - X_0)}],
\end{equation*}
em que:
\begin{align*}
    X \in \powerset(S) & \text{ é um conjunto de características;} \\
    X_0 \in \powerset (S) & \text{ é o conjunto de custo mínimo;} \\
    c_{max} \in \fieldR & \text{ é uma constante que escala o custo máximo de um subconjunto;} \\
    {\mathbf W} & \text { é uma matriz positiva-definida de pesos que dá forma a função.}
\end{align*}

A função de custo acima é decomponível em curvas em U. Podemos então adicionar à mesma um ruído senoidal, para esse fim generalizando a função com o acréscimo de um termo:
\begin{equation} \label{cost_function:esmaeil}
    c(X | X_0, {\mathbf W}, c_{max}) = c_{max} [1 - e^{- \frac{1}{2} (X - X_0)^T {\mathbf W} (X - X_0)}]  + A \cos (2\pi f \beta (X)),
\end{equation}
onde  $\beta (X) = \frac{1}{n} \sum_{i = 1}^{n} X (i)$ e $A \in \fieldR$ é uma constante que regula a amplitude do ruído, 
ou seja, a profundidade dos mínimos locais; e $f \in \fieldR$ controla a
frequência do ruído, isto é, qual a distância entre os mínimos locais 
gerados.

A função de custo da equação~\ref{cost_function:esmaeil} foi utilizada por Atashpaz-Gargari e colegas para avaliar a robustez de um algoritmo para o problema U-curve~\cite{AG+18}. Esta função já encontra-se implementada no arcabouço featsel e será utilizada em experimentos computacionais futuros.


