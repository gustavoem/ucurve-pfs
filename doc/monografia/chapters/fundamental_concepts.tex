% Contextualização e conceitos fundamentais
% .1 O problema de seleção de características
% .2 Funções de custo para esse problema
% .3 Redução para o problema de curvas em U

\section{O problema de seleção de características}
A seleção de características é um problema de otimização combinatória 
em que procuramos o melhor subconjunto de um conjunto de características
$S$. O espaço de busca desse problema é o conjunto potência de $S$, 
$\powerset (S)$, que é a coleção de todos os subconjuntos possíveis de
 $S$. A função de custo desse problema é uma função $c : \powerset (S) 
\to \fieldR_{+}$.

\begin{mydefinition}[Problema de seleção de características] Seja $S$
um conjunto de características, finito e não vazio, e $c$ uma função de 
custo. Encontrar $X \in \powerset (S)$ tal que $c (X) \leq c (Y)$,
$\forall Y \in \powerset (S)$.
\end{mydefinition}

O espaço de busca do problema de seleção de características possui uma
relação de ordem parcial definida pela relação $\subseteq$, portanto
este conjunto é {\bf parcialmente ordenado (poset)}.

\begin{mydefinition}
Uma {\bf \em cadeia} do reticulado booleano é uma sequência $X_1$, 
$X_2$, ..., $X_l$ tal que $X_1 \subseteq X_2 \subseteq \dots 
\subseteq X_l$.
\end{mydefinition}


%No contexto de aprendizado de máquina, é comum que as funções de custo
%utilizadas na seleção de característica descrevam curvas próximas do 
%formato de u nas cadeias do reticulado. Esse fenômeno é conhecido em 
%aprendizado e explicaremos como ele ocorre na seção 
%\ref{fund_concept:cost_functions}.

\section{Funções de custo}
Nesta seção apresentaremos as duas funções de custo mais utilizadas 
durante este trabalho: a entropia condicional média (MCE) e a soma de 
subconjuntos. A primeira foi utilizada na seleção de modelos de
aprendizado, enquanto a segunda foi utilizada para criação e solução
de instâncias artificiais. 
%A função de soma de subconjuntos é 
%decomponível em curvas u e a MCE não, porém explicaremos como a última 
%função deve ter um formato parecido com a da curva em u.

\subsection{Custo de modelos de aprendizado computacional} 
\label{fund_concept:cost_functions} A função de custo utilizada na 
solução do problema deve, de alguma forma, refletir a qualidade do 
conjunto de características avaliado. Por isso,
diferentes aplicações de seleção de características
podem ter diferentes funções de custo. No contexto de aprendizado de 
máquina, uma possível função de custo é a entropia condicional média
(MCE), que já foi utilizada por exemplo na construção de W-operadores
~\cite{MJCJB06}.

\begin{mydefinition}\label{def:conditional_entropy}
Dado um problema de aprendizado em que $Y$ é o conjunto de possíveis
rótulos e $W = (w_1, ..., w_n)$, com $w_i \in A_i$, é o conjunto de
variáveis. Seja $W' = (w_{I(1)}, w_{I(2)}, ..., w_{I(k)})$ um conjunto 
de variáveis (características) escolhidas, $\mathbf{X}$ uma vetor 
aleatório de tamanho $k$ com ${X_j} \in A_{I(j)}$, e $log0 = 0$. Então,
a {\bf \em entropia condicional} de $Y$ dado $\mathbf{X} = \mathbf x$ é:

\begin{center}
$
\begin{aligned}
H (Y | \mathbf{X = x}) = - 
\sum_{y \in Y} \probability (Y = y | \mathbf{X = x}) log \probability (Y = y | \mathbf{X = x})
\end{aligned}
$
\end{center}
\end{mydefinition}

\begin{mydefinition}
Sob o mesmo contexto definido em \ref{def:conditional_entropy}, 
definimos a {\bf \em entropia condicional média} como:
\begin{center}
$
\begin{aligned}
    \expectation[H (Y | \mathbf{X})] = 
    \sum_{\mathbf{x} \in \mathbf{X}} H (Y | \mathbf{X = x}) \probability (\mathbf{X = x})
\end{aligned}
$
\end{center}
\end{mydefinition}

%Vamos usar esta função como exemplo para entender intuitivamente como
%as funções de custo no problema de seleção de características descrevem
%curvas em u.

A função $H$, em teoria da informação, mede o inverso da quantidade 
média de informação que uma variável tem. Esta função atinge valor 
máximo quando a distribuição de probabilidade da variável aleatória em
questão é uniforme (todos valores que ela pode assumir são 
equiprováveis), e tem valores baixos quando essa distribuição é 
concentrada. 

Problemas de aprendizado em que os rótulos tem uma distribuição 
concentrada são mais fáceis do que os problemas em que essa distribuição
é menos concentrada. Tome como exemplo o problema de
classificar o lançamento de uma moeda $\mathbf{x}$ em $y$ (cara ou 
coroa); se toda moeda $\mathbf{x}$ é não viciada, então a distribuição
de $\probability (y | \mathbf{x})$ é pouco concentrada, por outro lado,
quando a moeda é viciada, a distribuição de 
$\probability (y | \mathbf{x})$ é concentrada e é mais fácil 
classificar este problema. Em termos mais formais, o erro do melhor 
classificador do problema mais fácil é menor do que o erro do melhor 
classificador do problema mais difícil.

Portanto, como a função $H$ é capaz de medir a concentração da 
distribuição de $Y$ dado $\mathbf{X = x}$, e quanto maior esta 
concentração mais fácil é o modelo de aprendizado, podemos dizer
que a função de custo $\expectation[H (Y | \mathbf{X})]$ pode 
representar a qualidade do modelo de classificação que usa o conjunto de
características de $\mathbf{X}$.

Agora, como já entendemos o funcionamento da função de custo
MCE e como ela se relaciona com a qualidade do conjunto de 
características avaliado, vamos entender o que acontece no modelo de 
aprendizado e na função de custo que usamos como exemplo quando 
percorremos uma cadeia do reticulado. 

Uma cadeia do poset pode ser vista como uma sequência de possíveis
escolhas de conjuntos de características ao qual a cada passo 
adicionamos uma característica. Isso significa que a cada passo dado
a variável $\mathbf{x}$ ganha uma componente a mais. Quando estamos no 
início da cadeia, poucas variáveis do problema são consideradas, 
portanto há uma grande abstração dos dados dos objetos sendo 
classificados, e conforme subimos uma cadeia, diminuímos a abstração dos
dados e isso faz com que a distribuição de $Y$ dado $\mathbf{x}$ se 
concentre.

Essa concentração da distribuição da probabilidade indica que o custo 
dos subconjuntos deve diminuir conforme subimos por uma cadeia do 
reticulado, ou seja, este raciocínio nos leva a pensar que adicionar 
características sempre melhora a classificação; de fato, o valor de
$\expectation[H (Y | \mathbf{X})]$ deve diminuir (até algum ponto 
de saturação) conforme aumentamos o número de variáveis do problema. 
Mas se isso é verdade, por que fazemos seleção de características? A 
inconsistência entre esse raciocínio e a motivação para seleção de 
característica é que essa linha de raciocínio negligenciou o fato de que 
problemas de classificação (supervisada) dependem de uma amostra da 
distribuição de $Y$ dado $\mathbf{X = x}$, ou seja, não sabemos nem ao 
menos calcular $H (Y | \mathbf{X = x})$, podemos apenas estimar o seu 
valor a partir da amostra.

A amostra da distribuição de $Y$ dado $\mathbf{X = x}$ é obtida do 
conjunto de treinamento do problema de aprendizado e quando o número
de amostras não é grande o suficiente a qualidade do classificador 
é comprometida. Além disso, o número de amostras necessárias deve
crescer conforme aumentamos a complexidade do modelo de aprendizado 
utilizado. Considerando que quando subimos uma cadeia do reticulado 
booleano estamos aumentando a complexidade do modelo, temos que, a
partir de um certo ponto, a qualidade do classificador que utiliza tal 
conjunto de características deve piorar. 

Portanto, é esperado que a função de custo descreva um formato de U nas 
cadeias do reticulado. No começo da cadeia, o custo deve diminuir por 
conta da maior granularidade dos dados de entrada, até algum ponto onde
a limitação no número de amostras combinada com o aumento da 
complexidade do modelo causem erros de estimação que aumentam o erro
do classificador criado em tal modelo.

No cálculo da entropia condicional média, o efeito do aumento da 
complexidade de $\mathbf X$ é a estimação ruim de 
$\probability (Y = y | \mathbf{X = x})$. Contorna-se este problema 
modificando a entropia condicional média para penalizar a entropia de 
$Y$ quando $\mathbf{x}$ foi observado poucas vezes. A função de custo
utilizada é, então:

\begin{center}
$
\begin{aligned}
    \hat{\expectation}[H (Y | \mathbf{X})] = \frac{N}{t}
    \sum_{\mathbf{x} \in \mathbf{X}} H (Y | \mathbf{X = x}) \probability (\mathbf{X = x})
\end{aligned}
$
\end{center}

\subsection{Soma de subconjuntos}
Para se avaliar o desempenho dos algoritmos criados neste trabalho, 
utilizamos instâncias artificiais que são reduções do problema da soma
de subconjuntos. Este problema consiste em, dado um conjunto finito de
inteiros não-negativos $S$ e um inteiro não-negativo $t$, descobrir se
há um subconjunto de $S$ que soma $t$. Podemos resolver este problema 
com a solução de uma instância do problema de seleção de características
onde o conjunto de características é $S'$ uma cópia de $S$ e a função de
custo é $c$:

\begin{equation*} \label{cost_function:subset_sum}
    c (X) = |t - \sum_{x \in X} x| \text{, para todo } 
                                        X \in \powerset(S') \text{.}
\end{equation*}

Assim como a função de custo MCE, a função de custo de somas de 
subconjuntos também apresenta um formato interessante nas cadeias
do reticulado booleano. Para toda cadeia com elementos $A \subseteq B 
\subseteq C$ vale que $c (B) \leq max\{c (A), c (B)\}$. Vamos provar 
esta propriedade para dois casos disjuntos, quando $|t - \sum_{b \in B}
b| > 0$ e quando $|t - \sum_{b \in B} b| \leq 0$. Começamos a 
demonstração definindo $D = B \setminus A$ e $E = C \setminus B$.

\begin{itemize}
    \item{se $|t - \sum_{b \in B} b| > 0$, então:}
    \begin{align*}
        c (B) & =  |t - \sum_{b \in B} b|  & \\
              & \leq  |t - \sum_{b \in B} b + \sum_{d \in D} d| & 
                \text{(pois $S$ contém apenas números positivos e $t -
                \sum_{b \in B} b > 0$)} \\
              & = |t - \sum_{a \in B \setminus D} a| \\
              & = |t - \sum_{a \in A} a| \\
              & = c (A)
    \end{align*}
    portanto, $c (B) \leq  c (A)$, logo $c (B) \leq max \{c (A), c (C)\}$.
    
    \item{se $|t - \sum_{b \in B} b| \leq 0$, então:}
    \begin{align*}
        c (B) & =  |t - \sum_{b \in B} b|  & \\
              & \leq  |t - \sum_{b \in B} b - \sum_{e \in E} e| & 
                \text{(pois $S$ contém apenas números positivos e $t -
                \sum_{b \in B} b \leq 0$)} \\
              & = |t - \sum_{c \in B \cup E} c| \\
              & = |t - \sum_{c \in C} c| \\
              & = c (C)
    \end{align*}
    portanto, $c (B) \leq  c (C)$, logo $c (B) \leq max \{c (A), c (C)\}$.
\end{itemize}
Como acabamos de provar para os dois casos possíveis, temos que 
$c (B) \leq max \{c (A), c (C)\}$. \qed

\section{O problema U-Curve}
As duas funções de custo apresentadas na seção ~\ref{fund_concept:cost_functions}
descrevem curvas que tem um formato em U (a menos de oscilações) nas 
cadeias do reticulado booleano, vamos definir esta propriedade agora.

\begin{mydefinition}
Uma cadeia é dita {\bf \em maximal} se não existe outra cadeia no 
reticulado que contenha propriamente esta cadeia.
\end{mydefinition}

\begin{mydefinition}\label{fund_concepts:ushape}
Uma função de custo $c$ é dita {\bf \em decomponível em curvas U} se
para toda cadeia maximal $X_1, ..., X_l$, $c(X_j) \leq max \{c (X_i),
c (X_k)\}$ sempre que $X_i \subseteq X_j \subseteq X_k$, $i, j, k \in 
\{1, ..., l\}$.
\end{mydefinition}

Vamos considerar então o problema de seleção de características em que a
função de custo utilizada é decomponível em curvas U. Este é o problema 
central deste trabalho.

\begin{mydefinition}[Problema U-Curve]
Dados um conjunto finito e não-vazio $S$ e uma função de custo $c$ 
decomponível em curvas em U, encontrar um subconjunto $X \in 
\powerset (S)$ tal que $c(X) \leq c(Y)$,  $\forall Y \in \powerset (S)$.
\end{mydefinition}

O problema U-Curve é um caso particular do problema de seleção de 
características com uma propriedade que nos permite achar o mínimo
global sem a necessidade de avaliar cada ponto do reticulado booleano. 
Isso é possível porque a propriedade U-Curve (da decomponibilidade da 
função de custo em curvas U) nos garante que o custo dos elementos de 
uma cadeia não podem cair uma vez que aumentaram. Sejam por exemplo
dois elementos $A \subseteq B$ de $\powerset (S)$, então:
\begin{itemize}
    \item{se $c(B) > c (A)$, então $c (X) > c (A)$ para todo $X$
        do intervalo $[B, \powerset (S)]$;}
    \item{se $c(A) > c (B)$, então $c (X) > c (B)$ para todo $X$ 
        do intervalo $[\emptyset, A]$;}
\end{itemize}

Desta maneira, quando um problema de seleção de características tem uma 
função de custo decomponível em curvas U a menos de algumas oscilações,
é vantajoso aproximar a solução deste problema pela solução encontrada
por um algoritmo de busca do problema U-Curve. Tal abordagem não é 
ótima, porém, como existem poucas oscilações da função de custo, é 
provável que a solução encontrada ainda seja próxima da melhor solução.


